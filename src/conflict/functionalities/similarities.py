# -*- coding: utf-8 -*-
"""similarities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EVdjMBE_uZzIB-daO7xQmlqfrizIXt35
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import euclidean_distances
import numpy as np
from collections import Counter
from collections import Counter
import re
import numpy as np

epsilon = 1e-20

"""### **Jaccard Similarities**"""

def jaccard_similarity(sentence1, sentence2):
    # Tokenize the sentences into sets of words
    words1 = set(sentence1.split())
    words2 = set(sentence2.split())

    # Calculate the Jaccard similarity
    intersection = len(words1.intersection(words2))
    union = len(words1) + len(words2) - intersection

    if union == 0:
        return 0.0  # To handle cases where both sets are empty
    else:
        return round(float(intersection) / (union + epsilon), 3)

# Example usage:
sentence1 = "This is a sample."
sentence2 = "Here is an example."
score = jaccard_similarity(sentence1, sentence2)
print(f"Jaccard Similarity: {score}")

"""### **Euclidean Distance**"""

def euclidean_distance(sentence1, sentence2):
    # Create a TF-IDF vectorizer
    vectorizer = TfidfVectorizer()

    # Fit and transform the sentences into TF-IDF vectors
    tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])

    # Calculate the Euclidean distance
    distance = euclidean_distances(tfidf_matrix)

    # The Euclidean distance matrix will have the distance between the two sentences
    return round(distance[0][1], 3)

# Example usage:
sentence1 = "This is the sentence."
sentence2 = "This is the sentence."
score = euclidean_distance(sentence1, sentence2)
print("Euclidean Distance:", score)

"""### **Levenshtein Distance**"""

def levenshtein_distance(s1, s2):
    # Create a matrix to store the distances
    matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]

    # Initialize the first row and column of the matrix
    for i in range(len(s1) + 1):
        matrix[i][0] = i
    for j in range(len(s2) + 1):
        matrix[0][j] = j

    # Fill in the matrix
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            cost = 0 if s1[i - 1] == s2[j - 1] else 1
            matrix[i][j] = min(matrix[i - 1][j] + 1,      # Deletion
                               matrix[i][j - 1] + 1,      # Insertion
                               matrix[i - 1][j - 1] + cost)  # Substitution

    # The final value in the matrix is the Levenshtein distance
    return round(matrix[len(s1)][len(s2)], 3)

# Example usage
sentence1 = "This is the sentence."
sentence2 = "This is the sentence."
distance = levenshtein_distance(sentence1, sentence2)
print("Levenshtein Distance:", distance)

"""### **Jensen Shannon Divergence**"""

def jensen_shannon_divergence(sentence1, sentence2):
    # Tokenize sentences into words
    words1 = sentence1.split()
    words2 = sentence2.split()

    # Calculate word frequencies for each sentence
    word_freq1 = Counter(words1)
    word_freq2 = Counter(words2)

    # Create a set of all unique words from both sentences
    all_words = set(words1).union(set(words2))

    # Calculate the probability distribution for each sentence
    prob_dist1 = [word_freq1[word] / (len(words1) + epsilon) for word in all_words]
    prob_dist2 = [word_freq2[word] / (len(words2) + epsilon) for word in all_words]

    # Ensure that the distributions sum to 1 and add epsilon to prevent divide-by-zero
    prob_dist1 = (np.array(prob_dist1)) / (np.sum(prob_dist1) + epsilon)
    prob_dist2 = (np.array(prob_dist2)) / (np.sum(prob_dist2) + epsilon)

    # Calculate the average distribution
    m = 0.5 * (prob_dist1 + prob_dist2)

    # Calculate the Kullback-Leibler divergences
    kl_divergence1 = np.sum(prob_dist1 * (np.log2(prob_dist1 / (m + epsilon) + epsilon) + epsilon))
    kl_divergence2 = np.sum(prob_dist2 * (np.log2(prob_dist2 / (m + epsilon) + epsilon) + epsilon))

    # Calculate the Jensen-Shannon Divergence
    jensen_shannon = 0.5 * (kl_divergence1 + kl_divergence2)

    return round(jensen_shannon, 3)

# Example usage
sentence1 = "This is the first sentence"
sentence2 = "This is the sentence"

# Calculate Jensen-Shannon Divergence
jsd_score = jensen_shannon_divergence(sentence1, sentence2)

print(f"Jensen-Shannon Divergence: {jsd_score}")

"""### **N-Gram Overlap**"""

def ngram_overlap(sentence1, sentence2):
    n = 3  # You can choose the value of N as per your requirement
    # Tokenize the input sentences into N-grams
    def ngrams(text, n):
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove punctuation
        words = text.split()
        ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]
        return ngrams

    ngrams1 = ngrams(sentence1, n)
    ngrams2 = ngrams(sentence2, n)

    # Count the occurrences of N-grams in each sentence
    count1 = Counter(ngrams1)
    count2 = Counter(ngrams2)

    # Calculate the intersection and union of N-grams
    intersection = sum((count1 & count2).values())
    union = sum((count1 | count2).values())

    # Calculate the N-gram overlap score
    if union == 0:
        score = 0.0
    else:
        score = intersection / (union + epsilon)

    return round(score, 3)

# Example usage:
sentence1 = "The quick brown fox jumps over the lazy dog"
sentence2 = "The quick brown"

overlap_score = ngram_overlap(sentence1, sentence2)
print(f"N-gram Overlap Score: {overlap_score}")

"""### **BM25**"""

def bm25_similarity(sentence1, sentence2):

    # Tokenize the sentences into words
    words1 = sentence1.split()
    words2 = sentence2.split()

    # Calculate the length of the sentences
    len1 = len(words1)
    len2 = len(words2)

    # Create a set of unique words for the sentences
    unique_words1 = set(words1)
    unique_words2 = set(words2)

    # Calculate the document frequency of each unique word
    df = {}
    for word in unique_words1 | unique_words2:
        df[word] = 0
        if word in words1:
            df[word] += 1
        if word in words2:
            df[word] += 1

    # Calculate the average document length
    avg_len = (len1 + len2) / 2

    # Initialize the BM25 score
    score = 0.0

    # Set BM25 parameters
    k1 = 1.5  # Tunable parameter
    b = 0.75  # Tunable parameter

    # Calculate BM25 for each word in the intersection of words1 and words2
    for word in unique_words1 & unique_words2:
        idf = np.log((len1 - df[word] + 0.5) / (df[word] + 0.5 + epsilon))
        numerator = (len1 * 0.5) + len2
        denominator = ((len1 * (1 - b)) + len2) * (1 - k1 + (k1 * len1 / (avg_len + epsilon)))
        score += idf * (numerator / (denominator + epsilon))

    return round(score, 3)

sentence1 = "This is the first sentence."
sentence2 = "This is another sentence with some words."
score = bm25_similarity(sentence1, sentence2)
print("BM25 Similarity Score:", score)